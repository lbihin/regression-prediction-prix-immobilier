{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1f252e",
   "metadata": {},
   "source": [
    "# Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452de3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "pd.set_option('display.float_format', lambda x: '{:.2f}'.format(x))\n",
    "\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RANDOM_STATE, TARGET\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"./data/kaggle_train_set.csv\")\n",
    "\n",
    "# Supprimons les outliers\n",
    "large_property_index = X[(X.GrLivArea > 4500) | (X.LotArea >150_000)].index.to_list()\n",
    "X.drop(index=large_property_index, inplace=True)\n",
    "\n",
    "y = X.pop(TARGET)\n",
    "\n",
    "print(f\"\\nLa shape de X est {X.shape}\")\n",
    "print(f\"La shape de y est {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer, RobustScaler, StandardScaler, OrdinalEncoder\n",
    "\n",
    "neighboors_categories = ['MeadowV', 'IDOTRR', 'BrDale', 'Edwards', 'BrkSide', 'OldTown', 'Blueste', 'Sawyer', 'SWISU', 'NAmes', 'NPkVill', 'Mitchel', 'SawyerW', 'Gilbert', 'Blmngtn', 'NWAmes', 'CollgCr', 'ClearCr', 'Crawfor', 'Somerst', 'Timber', 'Veenker', 'StoneBr', 'NoRidge', 'NridgHt']\n",
    "quality_order = ['Fa', 'TA', 'Gd', 'Ex']\n",
    "\n",
    "numerical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('power_transformation', PowerTransformer(method='yeo-johnson')),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "categorical_encoding_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', OrdinalEncoder(categories=[neighboors_categories, quality_order, quality_order])),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_encoding_pipeline, ['Neighborhood', 'ExterQual', 'KitchenQual']),\n",
    "        ('numerical', numerical_pipeline, ['GarageCars', 'GrLivArea', '1stFlrSF']),\n",
    "        ('passthrough_selected', 'passthrough', ['OverallQual', 'YearBuilt'])\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = {\n",
    "    \"Ridge\": Ridge(random_state=RANDOM_STATE),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(loss='huber', random_state=RANDOM_STATE),\n",
    "    \"LGBM\": LGBMRegressor(verbose=0, random_state=RANDOM_STATE),\n",
    "    \"SVR\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('regressor', TransformedTargetRegressor(\n",
    "            regressor=model,\n",
    "            func=np.log1p, inverse_func=np.expm1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Validation croisée pour calculer la MAE\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    results[name] = -scores.mean()\n",
    "\n",
    "\n",
    "print(\"Résultats des modèles :\")\n",
    "for name, score in results.items():\n",
    "    print(f\"{name}: MAE = {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f094526",
   "metadata": {},
   "source": [
    "Gradient Boosting performe mieux que les autres. Gardons le pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5df36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    gb_params = dict(\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        loss=trial.suggest_categorical(\"loss\", ['squared_error', 'absolute_error', 'huber', 'quantile']),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "        max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    )\n",
    "    \n",
    "    # Créer le pipeline avec le modèle et le prétraitement\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('regressor', TransformedTargetRegressor(\n",
    "            regressor=GradientBoostingRegressor(random_state=RANDOM_STATE, **gb_params),\n",
    "            func=np.log1p, inverse_func=np.expm1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Validation croisée pour calculer la MAE\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "# Lancer l'optimisation avec Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Afficher les meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "print(\"Meilleurs hyperparamètres :\", best_params)\n",
    "print(\"Meilleurs scores: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# paramètres\n",
    "min_samples_for_cv = 5  # si < 5 samples => pas de CV locale, on utilise la prédiction globale\n",
    "top_k = 10              # afficher les top k gaps positifs\n",
    "\n",
    "# s'assurer que l'estimator global est entraîné\n",
    "estimator = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('regressor', TransformedTargetRegressor(\n",
    "            regressor=GradientBoostingRegressor(random_state=RANDOM_STATE, **best_params),\n",
    "            func=np.log1p, inverse_func=np.expm1\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "estimator.fit(X, y)\n",
    "\n",
    "# prédictions globales (fallback)\n",
    "global_preds = pd.Series(estimator.predict(X), index=X.index)\n",
    "\n",
    "rows = []\n",
    "for neigh, idx in X.groupby('Neighborhood').groups.items():\n",
    "    Xn, yn = X.loc[idx], y.loc[idx]\n",
    "    n = len(Xn)\n",
    "    if n == 0:\n",
    "        continue\n",
    "\n",
    "    # si pas assez de samples ou y constant => on n'effectue pas la CV locale\n",
    "    if n < min_samples_for_cv or yn.nunique() == 1:\n",
    "        mae = mean_absolute_error(yn, global_preds.loc[idx])\n",
    "        method = 'global_pred_no_cv' if n < min_samples_for_cv else 'global_pred_constant_y'\n",
    "    else:\n",
    "        cv = min(5, n)\n",
    "        try:\n",
    "            scores = cross_val_score(clone(estimator), Xn, yn, cv=cv, scoring='neg_mean_absolute_error')\n",
    "            mae = -scores.mean()\n",
    "            method = 'local_cv'\n",
    "        except Exception:\n",
    "            mae = mean_absolute_error(yn, global_preds.loc[idx])\n",
    "            method = 'fallback_global_pred'\n",
    "\n",
    "    rows.append({\n",
    "        'Neighborhood': neigh,\n",
    "        'n_samples': n,\n",
    "        'mae': mae,\n",
    "        'gap_vs_global_best': mae - study.best_value,\n",
    "        'method': method\n",
    "    })\n",
    "\n",
    "neigh_local_mae_df = pd.DataFrame(rows).sort_values('gap_vs_global_best', ascending=False).reset_index(drop=True)\n",
    "neigh_local_mae_df.set_index('Neighborhood', inplace=True)\n",
    "\n",
    "# quartiers avec gap positif triés par gap décroissant\n",
    "positive_gaps = neigh_local_mae_df[neigh_local_mae_df['gap_vs_global_best'] > 0].sort_values('gap_vs_global_best', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Neighborhoods avec gap positif (pire que study.best_value) :\")\n",
    "display(positive_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e87401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement sur l´ensemble du dataset\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "\n",
    "best_params = {'max_depth': 5, 'loss': 'huber', 'learning_rate': 0.015996481097870868, 'n_estimators': 385, 'subsample': 0.7656864272872252, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
    "\n",
    "estimator = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('regressor', TransformedTargetRegressor(\n",
    "            regressor=GradientBoostingRegressor(random_state=RANDOM_STATE, **best_params),\n",
    "            func=np.log1p, inverse_func=np.expm1))\n",
    "        ]\n",
    ")\n",
    "\n",
    "estimator.fit(X, y)\n",
    "\n",
    "# Load the data\n",
    "test = pd.read_csv(\"./data/kaggle_test_set.csv\")\n",
    "\n",
    "# Prédictions\n",
    "result = estimator.predict(test)\n",
    "\n",
    "# Créer le DataFrame de soumission avec \"ID\" comme index\n",
    "submission_df = pd.DataFrame(result, columns=[\"SalePrice\"], index=test.index)\n",
    "submission_df.index.name = \"ID\"  # Renommer l'index en \"ID\"\n",
    "\n",
    "# Sauvegarder le fichier CSV\n",
    "submission_df.to_csv(\"./data/submission.csv\")\n",
    "\n",
    "# Vérification\n",
    "check_df = pd.read_csv(\"./data/submission.csv\", index_col='ID')\n",
    "check_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from utils import sauvegarder_model\n",
    "\n",
    "competition = \"regression-prediction-prix-immobilier\"\n",
    "\n",
    "\n",
    "def all_status_complete(df):\n",
    "    return all(df['status'].isin(['SubmissionStatus.ERROR', 'SubmissionStatus.COMPLETE']))\n",
    "\n",
    "# Fonction pour relancer le subprocess et récupérer les données\n",
    "def relaunch_subprocess():\n",
    "    result = subprocess.run(\n",
    "        [\"kaggle\", \"competitions\", \"submissions\", \"-v\", \"-c\",  competition],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "\n",
    "now = str(datetime.now())\n",
    "# Timeout de 1 minute\n",
    "timeout = 60  # en secondes\n",
    "start_time = time.time()\n",
    "\n",
    "data = None\n",
    "\n",
    "if SUBMIT:= True:\n",
    "    file_path = \"./data/submission.csv\"\n",
    "    message = f\"timestamp: {now}, Utilisation des paramètres: {estimator.get_params()}\"\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [\"kaggle\", \"competitions\", \"submit\", \"-c\", competition, \"-f\", file_path, \"-m\", message],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    print(\"⚒️\", result.stdout)\n",
    "\n",
    "    if result.stderr != '':\n",
    "        print(result.stderr)\n",
    "    data = None\n",
    "\n",
    "    while True:\n",
    "        # Récupérer les données depuis le subprocess\n",
    "        stdout_data = relaunch_subprocess()\n",
    "        \n",
    "        # Convertir les données en DataFrame\n",
    "        data = pd.read_csv(io.StringIO(stdout_data), parse_dates=['date'])\n",
    "        \n",
    "        # Vérifier si tous les statuts sont complets\n",
    "        if all_status_complete(data):\n",
    "            break\n",
    "        \n",
    "        # Vérification du timeout\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(\"\")\n",
    "            raise RuntimeError(\"imeout atteint. Arrêt de la boucle.\")\n",
    "        \n",
    "        # Pause avant la prochaine vérification\n",
    "        time.sleep(20)  # Pause de 20 secondes\n",
    "\n",
    "    if data is not None and all_status_complete(data):\n",
    "        # Trouver la ligne avec la date la plus récente\n",
    "        most_recent = data.sort_values('date', ascending=False).iloc[0]\n",
    "        recent_score = most_recent['publicScore']\n",
    "        best_score = data['publicScore'].min()\n",
    "\n",
    "        if recent_score < best_score:\n",
    "            print(f\"🥳 Nouveau meilleur score : {recent_score:.5f}\")\n",
    "            sauvegarder_model(estimator, timestamp=now, only_latest=False)\n",
    "        else:\n",
    "            print(f\"❌ Bien essayé mais c´est moins bon. Score: {recent_score:.5f}. Meilleur score : {best_score:.5f}\")\n",
    "\n",
    "if not SUBMIT:\n",
    "    sauvegarder_model(estimator, timestamp=now, only_latest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b8f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression-prediction-prix-immobilier-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
